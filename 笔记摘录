在redis的服务器中，会至少存在三个队列： 
a.用于请求对象去重的集合，队列的名称为spider.name:dupefilter，其中spider.name就是我们自定义的spider的名字，下同。 
b.待抓取的request对象的有序集合，队列的名称为spider.name:requests 
c.保存提取到item的列表，队列的名称为spider.name:items 
d.可能存在存放初始url的集合或者是列表，队列的名称可能是spider.name:start_urls

内存监控：
MEMUSAGE_NOTIFY_MAIL = ['3081881935@qq.com']
MEMUSAGE_REPORT = True
MEMUSAGE_ENABLED = True
MEMUSAGE_LIMIT_MB = 2048
MEMDEBUG_ENABLED = True
MEMDEBUG_NOTIFY = []

EXTENSIONS = {
    'share_code.extensionsItem.SpiderOpenCloseLogging': 100,
    'share_code.extensionsTime.Latencies': None,
    'scrapy.contrib.memusage.MemoryUsage': 50,
    'scrapy.contrib.memdebug.MemoryDebugger': 60
}

基于形态相似距离的时间序列相似度计算  李中刘洋洋 
https://wenku.baidu.com/view/58dfefbc2b160b4e777fcf77.html


隐藏层大小：（输入大小+输出大小）*2/3

hive -e "select * from /sdbadmin/hadoop/input/900915.csv" >> res1.csv新建csv文件，在此之前先将hdfs数据导入hive。

启动hive：
Service mysqld start
hive --service metastore
hive


scrapy_redis原理

(https://blog.csdn.net/hjhmpl123/article/details/53292602)
scrapy-redis原理: 
1.spider解析下载器下载下来的response,返回item或者是links 
2.item或者links经过spidermiddleware的process_spider_out()方法，交给engine。 
3.engine将item交给itempipeline,将links交给调度器 
4.在调度器中，先将request对象利用scrapy内置的指纹函数，生成一个指纹对象 
5.如果request对象中的dont_filter参数设置为False,并且该request对象的指纹不在信息指纹的队列中，那么就把该request对象放到优先级的队列中 
6.从优先级队列中获取request对象，交给engine 
7.engine将request对象交给下载器下载，期间会通过downloadmiddleware的process_request()方法 
8.下载器完成下载，获得response对象，将该对象交给engine,期间会通过downloadmiddleware的process_response()方法 
9.engine将获得的response对象交给spider进行解析，期间会经过spidermiddleware的process_spider_input()方法 
10.从第一步开始循环



zookeeper 路由和负载均衡的实现。
在zookeeper中，一但服务器与zookeeper集群断开连接，znode节点已经不存在，此时通过注册相应的watcher机制，服务消费者能够第一时间获取服务提供者信息的变更。利用znode的特点和watcher机制将其作为动态注册和获取服务信息的配置中心，统一管理服务名称和其对应的服务器列表，能够实时的感知到后端服务器的状态，从而保持服务配置信息能够一致以及进行简单的扩容。
Zookeeper类似于一棵节点树，当服务提供者启动时，将服务器的名称、地址以节点的信息添加到配置中心，而服务消费中通过服务器名称以及配置中心来获得需要调用的服务节点下的服务地址，再利用负载均衡算法选取其中的某一台服务器进行调用。



HIVE 介绍
(https://mp.weixin.qq.com/s?__biz=MzIzODExMDE5MA==&mid=2694182433&idx=1&sn=687b754cddc7255026434c683f487ac0#rd)
（1）hive 是基于 Hadoop 的一个数据仓库工具，可以将结构化的数据文件映射为一张数据库表，并提供完整的 sql 查询功能，可以将 sql 语句转换为 MapReduce 任务进行运行。其优点是学习成本低，可以通过类 SQL 语句快速实现简单的 MapReduce 统计，不必开发专门的 MapReduce 应用，十分适合数据仓库的统计分析。
（2）Hive 是建立在 Hadoop 上的数据仓库基础构架。它提供了一系列的工具，可以用来进行数据提取转化加载（ETL），这是一种可以存储、查询和分析存储在 Hadoop 中的大规模数据的机制。Hive 定义了简单的类 SQL 查询语言，称为 HQL，它允许熟悉 SQL 的用户查询数据。同时，这个语言也允许熟悉 MapReduce 开发者的开发自定义的 mapper 和 reducer 来处理内建的 mapper 和 reducer 无法完成的复杂的分析工作。

