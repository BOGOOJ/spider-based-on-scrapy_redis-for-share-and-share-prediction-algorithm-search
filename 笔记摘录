在redis的服务器中，会至少存在三个队列： 
a.用于请求对象去重的集合，队列的名称为spider.name:dupefilter，其中spider.name就是我们自定义的spider的名字，下同。 
b.待抓取的request对象的有序集合，队列的名称为spider.name:requests 
c.保存提取到item的列表，队列的名称为spider.name:items 
d.可能存在存放初始url的集合或者是列表，队列的名称可能是spider.name:start_urls

内存监控：
MEMUSAGE_NOTIFY_MAIL = ['3081881935@qq.com']
MEMUSAGE_REPORT = True
MEMUSAGE_ENABLED = True
MEMUSAGE_LIMIT_MB = 2048
MEMDEBUG_ENABLED = True
MEMDEBUG_NOTIFY = []

EXTENSIONS = {
    'share_code.extensionsItem.SpiderOpenCloseLogging': 100,
    'share_code.extensionsTime.Latencies': None,
    'scrapy.contrib.memusage.MemoryUsage': 50,
    'scrapy.contrib.memdebug.MemoryDebugger': 60
}

基于形态相似距离的时间序列相似度计算  李中刘洋洋 
https://wenku.baidu.com/view/58dfefbc2b160b4e777fcf77.html


隐藏层大小：（输入大小+输出大小）*2/3

hive -e "select * from /sdbadmin/hadoop/input/900915.csv" >> res1.csv新建csv文件，在此之前先将hdfs数据导入hive。

启动hive：
Service mysqld start
hive --service metastore
hive


scrapy_redis原理

(https://blog.csdn.net/hjhmpl123/article/details/53292602)
scrapy-redis原理: 
1.spider解析下载器下载下来的response,返回item或者是links 
2.item或者links经过spidermiddleware的process_spider_out()方法，交给engine。 
3.engine将item交给itempipeline,将links交给调度器 
4.在调度器中，先将request对象利用scrapy内置的指纹函数，生成一个指纹对象 
5.如果request对象中的dont_filter参数设置为False,并且该request对象的指纹不在信息指纹的队列中，那么就把该request对象放到优先级的队列中 
6.从优先级队列中获取request对象，交给engine 
7.engine将request对象交给下载器下载，期间会通过downloadmiddleware的process_request()方法 
8.下载器完成下载，获得response对象，将该对象交给engine,期间会通过downloadmiddleware的process_response()方法 
9.engine将获得的response对象交给spider进行解析，期间会经过spidermiddleware的process_spider_input()方法 
10.从第一步开始循环



zookeeper 路由和负载均衡的实现。
在zookeeper中，一但服务器与zookeeper集群断开连接，znode节点已经不存在，此时通过注册相应的watcher机制，服务消费者能够第一时间获取服务提供者信息的变更。利用znode的特点和watcher机制将其作为动态注册和获取服务信息的配置中心，统一管理服务名称和其对应的服务器列表，能够实时的感知到后端服务器的状态，从而保持服务配置信息能够一致以及进行简单的扩容。
Zookeeper类似于一棵节点树，当服务提供者启动时，将服务器的名称、地址以节点的信息添加到配置中心，而服务消费中通过服务器名称以及配置中心来获得需要调用的服务节点下的服务地址，再利用负载均衡算法选取其中的某一台服务器进行调用。

